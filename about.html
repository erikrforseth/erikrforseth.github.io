---
layout: default
title: About this model.
---
A large number of people have developed models for predicting the outcomes of college basketball games. For those that have made their picks publicly available, <a href="http://www.thepredictiontracker.com/basketball.php">ThePredictionTracker</a> does a great service by tracking the live performance of each model over the course of the season. Unfortunately, it's difficult to do a direct comparison of models using the summary page on the Tracker. For one thing, each model has predicted a different subset of games (in many cases this is accidental -- schedules get modified and web scrapers don't pick up the changes -- but some models don't start making picks altogether until weeks or months into the season). Further, there are a few misprinted lines in the Tracker data. For example, the Tracker shows an opening line of -22 and a closing line of +64.5 for UCLA vs. Presbyterian on 11/19/2019 (the line closed at -23 or -23.5 depending on the book). 
<br><br>
Throughout the 2018-19 season, I'll try to update this page with some further analysis of the Tracker data. For what follows, I chose a subset of models which have made picks since the very beginning of the season, and I threw out games for which any of those models did not make a pick. In a (somewhat lazy) attempt to address misprinted lines, I filtered out any games for which the opening and closing lines differed by more than 5 points (it's very rare that this really happens). 
<br><br>
Results shown are as of 2018-12-12 for a set of 1358 games: 
<br>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: center;">
      <th>Mean Squared Error (MSE)</th>
      <th>Model</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>127.576</td>
      <td>Line</td>
    </tr>
    <tr>
      <td>128.614</td>
      <td>Opening Line</td>
    </tr>
    <tr>
      <td>129.644</td>
      <td>TeamRankings</td>
    </tr>
    <tr>
      <td>129.694</td>
      <td>Erik Forseth</td>
    </tr>
    <tr>
      <td>134.772</td>
      <td>Dokter Entropy</td>
    </tr>
    <tr>
      <td>136.316</td>
      <td>ESPN BPI</td>
    </tr>
    <tr>
      <td>139.424</td>
      <td>StatFox</td>
    </tr>
    <tr>
      <td>139.724</td>
      <td>Kenneth Massey</td>
    </tr>
    <tr>
      <td>140.907</td>
      <td>Sagarin Rating</td>
    </tr>
    <tr>
      <td>141.394</td>
      <td>Sagarin Golden Mean</td>
    </tr>
    <tr>
      <td>141.620</td>
      <td>Sagarin Predictor</td>
    </tr>
    <tr>
      <td>143.908</td>
      <td>DRatings.com</td>
    </tr>
    <tr>
      <td>146.245</td>
      <td>Sonny Moore</td>
    </tr>
    <tr>
      <td>160.341</td>
      <td>Sagarin Recent</td>
    </tr>
    <tr>
      <td>161.729</td>
      <td>ComPughter Ratings</td>
    </tr>
  </tbody>
</table>
<br>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: center;">
      <th>Accuracy</th>
      <th>Model</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0.779</td>
      <td>Line</td>
    </tr>
    <tr>
      <td>0.777</td>
      <td>Opening Line</td>
    </tr>
    <tr>
      <td>0.771</td>
      <td>Erik Forseth</td>
    </tr>
    <tr>
      <td>0.766</td>
      <td>TeamRankings</td>
    </tr>
    <tr>
      <td>0.765</td>
      <td>ESPN BPI</td>
    </tr>
    <tr>
      <td>0.764</td>
      <td>Sagarin Predictor</td>
    </tr>
    <tr>
      <td>0.763</td>
      <td>Sagarin Golden Mean</td>
    </tr>
    <tr>
      <td>0.758</td>
      <td>Sagarin Rating</td>
    </tr>
    <tr>
      <td>0.757</td>
      <td>Sonny Moore</td>
    </tr>
    <tr>
      <td>0.757</td>
      <td>DRatings.com</td>
    </tr>
    <tr>
      <td>0.755</td>
      <td>Dokter Entropy</td>
    </tr>
    <tr>
      <td>0.752</td>
      <td>Kenneth Massey</td>
    </tr>
    <tr>
      <td>0.750</td>
      <td>StatFox</td>
    </tr>
    <tr>
      <td>0.739</td>
      <td>Sagarin Recent</td>
    </tr>
    <tr>
      <td>0.737</td>
      <td>ComPughter Ratings</td>
    </tr>
  </tbody>
</table>
<br><br>
Clearly the line is the best statistical predictor of the outcome. Nevertheless, we can ask how each model would have done against the spread, shown below: 
<br>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: center;">
      <th>% Against the Spread</th>
      <th>Model</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0.517</td>
      <td>ESPN BPI</td>
    </tr>
    <tr>
      <td>0.512</td>
      <td>Sagarin Golden Mean</td>
    </tr>
    <tr>
      <td>0.512</td>
      <td>DRatings.com</td>
    </tr>
    <tr>
      <td>0.509</td>
      <td>Sagarin Rating</td>
    </tr>
    <tr>
      <td>0.507</td>
      <td>Sagarin Predictor</td>
    </tr>
    <tr>
      <td>0.504</td>
      <td>Sonny Moore</td>
    </tr>
    <tr>
      <td>0.502</td>
      <td>TeamRankings</td>
    </tr>
    <tr>
      <td>0.500</td>
      <td>Opening Line</td>
    </tr>
    <tr>
      <td>0.498</td>
      <td>Erik Forseth</td>
    </tr>
    <tr>
      <td>0.493</td>
      <td>StatFox</td>
    </tr>
    <tr>
      <td>0.490</td>
      <td>Kenneth Massey</td>
    </tr>
    <tr>
      <td>0.489</td>
      <td>Sagarin Recent</td>
    </tr>
    <tr>
      <td>0.480</td>
      <td>Dokter Entropy</td>
    </tr>
    <tr>
      <td>0.456</td>
      <td>ComPughter Ratings</td>
    </tr>
  </tbody>
</table>
<br><br>
 Given how sharp the line is, I personally find it a little hard to believe that any apparent "edge" seen here isn't just luck (sampling error). 
<br><br>
 Although no individual model predicts the point spread as well as the line, we might ask whether any linear combination of models can do so. Let's regress the observed margins of victory onto the predictions of each model, but constrain the regression to have nonnegative coefficients. Subject to the nonnegativity constraint, this would give the optimal (backward-looking) mixture of predictors. We find: 
<br>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: center;">
      <th>Coefficient</th>
      <th>Model</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0.438</td>
      <td>Erik Forseth</td>
    </tr>
    <tr>
      <td>0.318</td>
      <td>ESPN BPI</td>
    </tr>
    <tr>
      <td>0.121</td>
      <td>Dokter Entropy</td>
    </tr>
    <tr>
      <td>0.083</td>
      <td>TeamRankings</td>
    </tr>
    <tr>
      <td>0.050</td>
      <td>StatFox</td>
    </tr>
    <tr>
      <td>0.037</td>
      <td>Sagarin Predictor</td>
    </tr>
    <tr>
      <td>0.005</td>
      <td>Sonny Moore</td>
    </tr>
    <tr>
      <td>0.000</td>
      <td>Kenneth Massey</td>
    </tr>
    <tr>
      <td>0.000</td>
      <td>Sagarin Rating</td>
    </tr>
    <tr>
      <td>0.000</td>
      <td>Sagarin Recent</td>
    </tr>
    <tr>
      <td>0.000</td>
      <td>Sagarin Golden Mean</td>
    </tr>
    <tr>
      <td>0.000</td>
      <td>DRatings.com</td>
    </tr>
    <tr>
      <td>0.000</td>
      <td>ComPughter Ratings</td>
    </tr>
  </tbody>
</table> The MSE of this hypothetical predictor would be 127.623. Not bad! However, note that this is optimistic, since in addition to being backward-looking, we both fit the model and then computed the MSE using the full dataset. 
<br><br>
Out of curiosity, what if we included the line itself in the above regression? Can any of our models add value when combined with the line?<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: center;">
      <th>Coefficient</th>
      <th>Model</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0.552</td>
      <td>Line</td>
    </tr>
    <tr>
      <td>0.208</td>
      <td>ESPN BPI</td>
    </tr>
    <tr>
      <td>0.203</td>
      <td>Erik Forseth</td>
    </tr>
    <tr>
      <td>0.049</td>
      <td>StatFox</td>
    </tr>
    <tr>
      <td>0.021</td>
      <td>Sagarin Predictor</td>
    </tr>
    <tr>
      <td>0.013</td>
      <td>Dokter Entropy</td>
    </tr>
    <tr>
      <td>0.000</td>
      <td>Opening Line</td>
    </tr>
    <tr>
      <td>0.000</td>
      <td>Kenneth Massey</td>
    </tr>
    <tr>
      <td>0.000</td>
      <td>Sagarin Rating</td>
    </tr>
    <tr>
      <td>0.000</td>
      <td>Sonny Moore</td>
    </tr>
    <tr>
      <td>0.000</td>
      <td>TeamRankings</td>
    </tr>
    <tr>
      <td>0.000</td>
      <td>Sagarin Recent</td>
    </tr>
    <tr>
      <td>0.000</td>
      <td>Sagarin Golden Mean</td>
    </tr>
    <tr>
      <td>0.000</td>
      <td>DRatings.com</td>
    </tr>
    <tr>
      <td>0.000</td>
      <td>ComPughter Ratings</td>
    </tr>
  </tbody>
</table> 
<br>
Interestingly, it seems that a couple of the models do perhaps capture something the line does not. The hypothetical MSE of this mixture would be 126.674. 
<br><br>
At some point I will make my analysis available as a Jupyter notebook.
