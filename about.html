---
layout: default
title: About this model.
---
A large number of people have developed models for predicting the outcomes of college basketball games. For those that have made their picks publicly available, <a href="http://www.thepredictiontracker.com/basketball.php">ThePredictionTracker</a> does a great service by tracking the live performance of each model over the course of the season. Unfortunately, it's difficult to do a direct comparison of models using the summary page on the Tracker. For one thing, each model has predicted a different subset of games (in many cases this is accidental -- schedules get modified and web scrapers don't pick up the changes -- but some models don't start making picks altogether until weeks or months into the season). Further, there are a few misprinted lines in the Tracker data. For example, the Tracker shows an opening line of -22 and a closing line of +64.5 for UCLA vs. Presbyterian on 11/19/2019 (the line closed at -23 or -23.5 depending on the book). 
<br><br>
Throughout the 2018-19 season, I'll try to update this page with some further analysis of the Tracker data. For what follows, I chose a subset of models which have made picks since the very beginning of the season, and I threw out games for which any of those models did not make a pick. In a (somewhat lazy) attempt to address misprinted lines, I filtered out any games for which the opening and closing lines differed by more than 5 points (it's very rare that this really happens). 
<br><br>
Results shown are as of 2018-12-11 for a set of 1343 games: 
<br>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: center;">
      <th>Mean Squared Error (MSE)</th>
      <th>Model</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>127.278</td>
      <td>Line</td>
    </tr>
    <tr>
      <td>128.371</td>
      <td>Opening Line</td>
    </tr>
    <tr>
      <td>129.379</td>
      <td>Erik Forseth</td>
    </tr>
    <tr>
      <td>129.427</td>
      <td>TeamRankings</td>
    </tr>
    <tr>
      <td>134.465</td>
      <td>Dokter Entropy</td>
    </tr>
    <tr>
      <td>136.287</td>
      <td>ESPN BPI</td>
    </tr>
    <tr>
      <td>139.124</td>
      <td>StatFox</td>
    </tr>
    <tr>
      <td>139.282</td>
      <td>Kenneth Massey</td>
    </tr>
    <tr>
      <td>140.537</td>
      <td>Sagarin Rating</td>
    </tr>
    <tr>
      <td>140.824</td>
      <td>Sagarin Golden Mean</td>
    </tr>
    <tr>
      <td>141.324</td>
      <td>Sagarin Predictor</td>
    </tr>
    <tr>
      <td>143.026</td>
      <td>DRatings.com</td>
    </tr>
    <tr>
      <td>145.794</td>
      <td>Sonny Moore</td>
    </tr>
    <tr>
      <td>159.649</td>
      <td>Sagarin Recent</td>
    </tr>
    <tr>
      <td>161.641</td>
      <td>ComPughter Ratings</td>
    </tr>
  </tbody>
</table>
<br>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: center;">
      <th>Accuracy</th>
      <th>Model</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0.779</td>
      <td>Line</td>
    </tr>
    <tr>
      <td>0.777</td>
      <td>Opening Line</td>
    </tr>
    <tr>
      <td>0.771</td>
      <td>Erik Forseth</td>
    </tr>
    <tr>
      <td>0.767</td>
      <td>ESPN BPI</td>
    </tr>
    <tr>
      <td>0.767</td>
      <td>TeamRankings</td>
    </tr>
    <tr>
      <td>0.765</td>
      <td>Sagarin Predictor</td>
    </tr>
    <tr>
      <td>0.763</td>
      <td>Sagarin Golden Mean</td>
    </tr>
    <tr>
      <td>0.758</td>
      <td>Sagarin Rating</td>
    </tr>
    <tr>
      <td>0.757</td>
      <td>Sonny Moore</td>
    </tr>
    <tr>
      <td>0.757</td>
      <td>DRatings.com</td>
    </tr>
    <tr>
      <td>0.755</td>
      <td>Dokter Entropy</td>
    </tr>
    <tr>
      <td>0.752</td>
      <td>Kenneth Massey</td>
    </tr>
    <tr>
      <td>0.751</td>
      <td>StatFox</td>
    </tr>
    <tr>
      <td>0.739</td>
      <td>Sagarin Recent</td>
    </tr>
    <tr>
      <td>0.737</td>
      <td>ComPughter Ratings</td>
    </tr>
  </tbody>
</table>
<br><br>
Clearly the line is the best statistical predictor of the outcome. Nevertheless, we can ask how each model would have done against the spread, shown below: 
<br>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: center;">
      <th>% Against the Spread</th>
      <th>Model</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0.519</td>
      <td>Sagarin Predictor</td>
    </tr>
    <tr>
      <td>0.518</td>
      <td>Sagarin Golden Mean</td>
    </tr>
    <tr>
      <td>0.515</td>
      <td>Opening Line</td>
    </tr>
    <tr>
      <td>0.513</td>
      <td>Sonny Moore</td>
    </tr>
    <tr>
      <td>0.512</td>
      <td>Sagarin Rating</td>
    </tr>
    <tr>
      <td>0.510</td>
      <td>ESPN BPI</td>
    </tr>
    <tr>
      <td>0.510</td>
      <td>Erik Forseth</td>
    </tr>
    <tr>
      <td>0.510</td>
      <td>DRatings.com</td>
    </tr>
    <tr>
      <td>0.507</td>
      <td>StatFox</td>
    </tr>
    <tr>
      <td>0.506</td>
      <td>TeamRankings</td>
    </tr>
    <tr>
      <td>0.504</td>
      <td>Kenneth Massey</td>
    </tr>
    <tr>
      <td>0.503</td>
      <td>Dokter Entropy</td>
    </tr>
    <tr>
      <td>0.500</td>
      <td>Sagarin Recent</td>
    </tr>
    <tr>
      <td>0.496</td>
      <td>ComPughter Ratings</td>
    </tr>
  </tbody>
</table>
<br><br>
 Given how sharp the line is, I personally find it a little hard to believe that any apparent "edge" seen here isn't just luck (sampling error). 
<br><br>
 Finally, let's regress the observed margins of victory onto the predictions of each model, but constrain the regression to have nonnegative coefficients. Subject to the nonnegativity constraint, this would give the optimal (backward-looking) linear combination of predictors. We find: 
<br>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: center;">
      <th>Coefficient</th>
      <th>Model</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0.440</td>
      <td>Erik Forseth</td>
    </tr>
    <tr>
      <td>0.303</td>
      <td>ESPN BPI</td>
    </tr>
    <tr>
      <td>0.121</td>
      <td>Dokter Entropy</td>
    </tr>
    <tr>
      <td>0.079</td>
      <td>TeamRankings</td>
    </tr>
    <tr>
      <td>0.053</td>
      <td>StatFox</td>
    </tr>
    <tr>
      <td>0.040</td>
      <td>Sagarin Predictor</td>
    </tr>
    <tr>
      <td>0.011</td>
      <td>Sonny Moore</td>
    </tr>
    <tr>
      <td>0.000</td>
      <td>Kenneth Massey</td>
    </tr>
    <tr>
      <td>0.000</td>
      <td>Sagarin Rating</td>
    </tr>
    <tr>
      <td>0.000</td>
      <td>Sagarin Recent</td>
    </tr>
    <tr>
      <td>0.000</td>
      <td>Sagarin Golden Mean</td>
    </tr>
    <tr>
      <td>0.000</td>
      <td>DRatings.com</td>
    </tr>
    <tr>
      <td>0.000</td>
      <td>ComPughter Ratings</td>
    </tr>
  </tbody>
</table> The MSE of this hypothetical predictor would be 127.408. Note that this is optimistic, since we both fit the model and then computed the MSE using the full dataset.
<br><br>
At some point I will make my analysis available as a Jupyter notebook.
