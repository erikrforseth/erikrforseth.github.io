---
layout: default
title: Erik Forseth
---
<div class="blurb">
  <h2>About the model</h2>
  <p>
    My model is a linear regression formula trained on 20,000+ regular season games from 2011-2015, which takes as input a number of season-to-date team statistics, some raw (e.g. average margin of victory) and some adjusted according to the team's opponents' statistics. For these adjusted measures, I use something of a generalization of Andrew Dolphin's "infinite" RPI routine, described <a href="http://netprophetblog.blogspot.com/2011/04/infinitely-deep-rpi.html">here</a> and <a href="/basketball/RPInotes.pdf">here</a>. For most of the analysis that goes into this, I use <a href="http://pandas.pydata.org/">pandas</a>, <a href="http://statsmodels.sourceforge.net/">statsmodels</a>, and <a href="http://scikit-learn.org/stable/">scikit-learn</a>, though I wrote my own routines for things like feature selection.
  </p>
  <p>
    Things have come a long way since my initial foray into all this last season, but I am always still tinkering with the model in an attempt to bat down errors and improve win/loss percentages. College basketball outcomes are inherently noisy, so what actually constitutes a "good" predictor? A look at the results from last season on <a href="http://www.thepredictiontracker.com/bbresults.php">The Prediction Tracker</a> gives a pretty good idea: a good predictor correctly chooses the outcome (win vs. loss) of the game about 71-73% of the time, and can predict the margin of victory (MOV) with a mean square error (MSE) of ~115 and a mean absolute error (MAE) of ~8.5. My predictor last year was mediocre at picking the outcome (70%), but excellent at predicting the margin of victory (MSE = ~111). Though I've made some big strides and now have a better and more robust model, there's actually a little more to the story when looking at these performance measures. For example, it tends to be difficult to accurately predict the MOV for early season games (there isn't much data to go on by that point, and besides, early-season matchups can make for some noisy blowouts), but relatively easy to choose the outright result of these games. On the other hand, mid/late-season games are a little easier to predict when it comes to MOV, but a greater number of close games means it's generally tougher to predict the winner or loser. 
  </p>
  <p>
    Therefore, it's of course possible to "enhance" your model's performance by choosing which games to predict and which games to leave alone. One might ignore early season games entirely. Or, one might skip expected blowouts, or certain small-conference games, etc. The Tracker only gives a snapshot of performance for a certain subset of games, but doesn't say anything about that subset except for its size (it shows the total number of games predicted). Indeed, if I'd begun submitting picks to the Tracker earlier in the season (I only became aware of the site around early January), the MSE shown for my model look a bit worse, but the straight up percentage would look a couple of points better. 
  </p>
  <p>
    While knowing a priori where you have an advantage and where you don't is certainly important, it's my view that constructing a model that does well in <i>all</i> circumstances is the most exciting challenge about this whole endeavor. I would argue that when assessing the performance of the various predictors next season, it's important to consider the total number of games that each one has logged. The truly impressive predictors will boast low error measures and high win/loss rates across the largest set of games. 
  </p>

</div><!-- /.blurb -->
